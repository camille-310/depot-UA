{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camille-310/depot-UA/blob/main/02_Exe_MapReduce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>MapReduce Mini-Project: Analyzing Amazon Movie Reviews</h2>\n",
        "\n",
        "<p>\n",
        "In this exercise, you will work as a data engineer for a streaming platform.\n",
        "Your goal is to perform several analytics tasks on a free and publicly\n",
        "available dataset of Amazon Movie Reviews using MapReduce in Hadoop.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "You will complete four tasks:\n",
        "</p>\n",
        "\n",
        "<ol>\n",
        "  <li><b>Count total number of reviews per movie</b></li>\n",
        "  <li><b>Compute average rating per movie</b></li>\n",
        "  <li><b>Extract frequent keywords from reviews</b></li>\n",
        "  <li><b>Join average ratings with top keywords</b></li>\n",
        "</ol>\n",
        "\n",
        "<p>\n",
        "For each task, you will write a MapReduce program (Python Streaming or Java)\n",
        "and run it using Hadoop in local mode. Your final outputs will help the\n",
        "company understand which movies are popular, how viewers rate them, and what\n",
        "keywords often appear in the reviews.\n",
        "</p>"
      ],
      "metadata": {
        "id": "HQYoGI8srrmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>About the Dataset</h2>\n",
        "\n",
        "<p>\n",
        "We will use the <b>Amazon Movies &amp; TV 5-core dataset</b>, which is publicly\n",
        "available and contains movie reviews from Amazon. Each entry in the dataset\n",
        "is stored as a JSON object with fields such as:\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "  <li><code>reviewerID</code> – the ID of the reviewer</li>\n",
        "  <li><code>asin</code> – unique movie identifier</li>\n",
        "  <li><code>reviewText</code> – full written review</li>\n",
        "  <li><code>overall</code> – the star rating (1 to 5)</li>\n",
        "  <li><code>vote</code> – how many users found the review helpful</li>\n",
        "  <li><code>category</code> – always “Movies &amp; TV” in this dataset</li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "You will download the dataset and inspect a few records to understand its\n",
        "structure before starting the tasks.\n",
        "</p>"
      ],
      "metadata": {
        "id": "gZpGhPQ3tBhf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jk2_fRXfrmbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cab9805-b6e9-4ab2-c422-27f07d38f4a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading SMALL Movies & TV 5-core dataset...\n",
            "--2025-11-24 14:47:27--  https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Movies_and_TV_5.json.gz\n",
            "Resolving jmcauley.ucsd.edu (jmcauley.ucsd.edu)... 137.110.160.73\n",
            "Connecting to jmcauley.ucsd.edu (jmcauley.ucsd.edu)|137.110.160.73|:443... connected.\n",
            "WARNING: cannot verify jmcauley.ucsd.edu's certificate, issued by ‘CN=InCommon RSA Server CA 2,O=Internet2,C=US’:\n",
            "  Unable to locally verify the issuer's authority.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 791322468 (755M) [application/x-gzip]\n",
            "Saving to: ‘Movies_and_TV_small.json.gz’\n",
            "\n",
            "Movies_and_TV_small 100%[===================>] 754.66M  23.7MB/s    in 23s     \n",
            "\n",
            "2025-11-24 14:47:50 (32.6 MB/s) - ‘Movies_and_TV_small.json.gz’ saved [791322468/791322468]\n",
            "\n",
            "Download complete.\n",
            "\n",
            "Loading JSON data from JSON Lines format...\n",
            "Total records loaded: 3410019\n",
            "\n",
            "Converting to JSON-lines format (outputting to movies.json with 900,000 records)...\n",
            "Conversion complete. Saved as movies.json with 900000 records\n",
            "\n",
            "Sample entries:\n",
            "\n",
            "{'overall': 5.0, 'verified': True, 'reviewTime': '11 9, 2012', 'reviewerID': 'A2M1CU2IRZG0K9', 'asin': '0005089549', 'style': {'Format:': ' VHS Tape'}, 'reviewerName': 'Terri', 'reviewText': \"So sorry I didn't purchase this years ago when it first came out!!  This is very good and entertaining!  We absolutely loved it and anticipate seeing it repeatedly.  We actually wore out the cassette years back, so we also purchased this same product on cd.  Best purchase we made out of all!  Would purchase on dvd if we could find one.\", 'summary': 'Amazing!', 'unixReviewTime': 1352419200}\n",
            "{'overall': 5.0, 'verified': True, 'reviewTime': '12 30, 2011', 'reviewerID': 'AFTUJYISOFHY6', 'asin': '0005089549', 'style': {'Format:': ' VHS Tape'}, 'reviewerName': 'Melissa D. Abercrombie', 'reviewText': 'Believe me when I tell you that you will receive a blessing watching this video of the Cathedral Quartet.  They bring back most of the singers that were ever in their group and it is a really great VHS.', 'summary': 'Great Gospel VHS of the Cathedrals!', 'unixReviewTime': 1325203200}\n",
            "{'overall': 5.0, 'vote': '11', 'verified': True, 'reviewTime': '04 21, 2005', 'reviewerID': 'A3JVF9Y53BEOGC', 'asin': '000503860X', 'style': {'Format:': ' DVD'}, 'reviewerName': 'Anthony Thompson', 'reviewText': \"I have seen X live many times, both in the early days and their more recent reunion shows. Trust me when I say this: they never disappoint as a live band! This DVD document of a show on their home turf of LA is a dream come true. Can't wait.\", 'summary': 'A great document of a great band', 'unixReviewTime': 1114041600}\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import json\n",
        "import os\n",
        "import sys # Import sys for printing warnings to stderr\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1) Download the SMALL Movies & TV dataset (correct version)\n",
        "# -------------------------------------------------------------------\n",
        "print(\"Downloading SMALL Movies & TV 5-core dataset...\")\n",
        "\n",
        "URL = \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Movies_and_TV_5.json.gz\"\n",
        "FILE_GZ = \"Movies_and_TV_small.json.gz\"\n",
        "\n",
        "!wget --no-check-certificate -O {FILE_GZ} {URL}\n",
        "\n",
        "if os.path.getsize(FILE_GZ) == 0:\n",
        "    raise ValueError(\"Downloaded file is empty!\")\n",
        "\n",
        "print(\"Download complete.\\n\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2) Load JSON data (each line is a JSON object)\n",
        "# -------------------------------------------------------------------\n",
        "print(\"Loading JSON data from JSON Lines format...\")\n",
        "\n",
        "data = []\n",
        "with gzip.open(FILE_GZ, \"rt\", encoding=\"utf-8\") as f:\n",
        "    for line_num, line in enumerate(f, 1):\n",
        "        line = line.strip()\n",
        "        if line: # Only process non-empty lines\n",
        "            try:\n",
        "                data.append(json.loads(line))\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Warning: Could not decode JSON on line {line_num}: {line}. Error: {e}\", file=sys.stderr)\n",
        "                # Continue to the next line to be robust against malformed lines\n",
        "                continue\n",
        "\n",
        "print(f\"Total records loaded: {len(data)}\") # Should be ~3.4 million records\n",
        "print()\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3) Convert to JSON-LINES format for MapReduce (if not already done)\n",
        "#    This step ensures 'movies.json' is a clean JSON-Lines file.\n",
        "# -------------------------------------------------------------------\n",
        "print(\"Converting to JSON-lines format (outputting to movies.json with 900,000 records)...\")\n",
        "\n",
        "# Limit to 900,000 records to have less runing times on colab (on a real cluster, remove this line)\n",
        "limited_data = data[:900000]\n",
        "\n",
        "with open(\"movies.json\", \"w\", encoding=\"utf-8\") as out:\n",
        "    for entry in limited_data:\n",
        "        out.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "print(f\"Conversion complete. Saved as movies.json with {len(limited_data)} records\\n\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 4) Preview\n",
        "# -------------------------------------------------------------------\n",
        "print(\"Sample entries:\\n\")\n",
        "\n",
        "with open(\"movies.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for i in range(3):\n",
        "        line = f.readline()\n",
        "        if not line: # Check for end of file\n",
        "            print(\"Not enough lines in movies.json to display 3 samples.\")\n",
        "            break\n",
        "        print(json.loads(line))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
        "!tar -xzf hadoop-3.3.6.tar.gz"
      ],
      "metadata": {
        "id": "LlZlfmOeVZsE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/content/hadoop-3.3.6\"\n",
        "os.environ[\"PATH\"] += f\":{os.environ['HADOOP_HOME']}/bin:{os.environ['HADOOP_HOME']}/sbin\""
      ],
      "metadata": {
        "id": "GGLweFKfViJT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > /content/hadoop-3.3.6/etc/hadoop/core-site.xml << EOF\n",
        "<configuration>\n",
        " <property>\n",
        "  <name>fs.defaultFS</name>\n",
        "  <value>file:///</value>\n",
        " </property>\n",
        "</configuration> EOF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIPN2G35VpeS",
        "outputId": "e67074c5-a016-4bca-a8a8-137d25fa2001"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "bash: line 7: warning: here-document at line 1 delimited by end-of-file (wanted `EOF')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Task 1 — Count Total Number of Reviews per Movie</h2>\n",
        "\n",
        "<p>\n",
        "Your first task is to count how many reviews each movie has received. You will\n",
        "write a MapReduce program where:\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "  <li>The <b>mapper</b> reads each JSON record, extracts the <code>asin</code>\n",
        "      field, and emits <code>(asin, 1)</code>.</li>\n",
        "  <li>The <b>reducer</b> sums the counts for each movie and outputs\n",
        "      <code>(asin, total_reviews)</code>.</li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "This task is conceptually similar to a word count, but applied to movie IDs.\n",
        "Complete the mapper and reducer code in the following cell.\n",
        "</p>"
      ],
      "metadata": {
        "id": "qWlGRRlYtMwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your Mapper and Reducer code for Task 1 here.\n",
        "# You may use Python Hadoop Streaming or Java MapReduce."
      ],
      "metadata": {
        "id": "imp3lubewAy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper1.py\n",
        "#!/usr/bin/env python3\n",
        "import sys,json\n",
        "\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    if not line:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        fichier = json.loads(line)\n",
        "        asin = fichier.get(\"asin\", None)\n",
        "        if asin:\n",
        "            print(f\"{asin}\\t1\")\n",
        "    except json.JSONDecodeError:\n",
        "        continue"
      ],
      "metadata": {
        "id": "9tH9bEAEWHSv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer1.py\n",
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "\n",
        "current_asin = None\n",
        "count = 0\n",
        "\n",
        "for line in sys.stdin:\n",
        "    asin, value = line.strip().split(\"\\t\")\n",
        "    value = int(value)\n",
        "\n",
        "    if asin != current_asin:\n",
        "        if current_asin is not None:\n",
        "            print(f\"{current_asin}\\t{count}\")\n",
        "        current_asin = asin\n",
        "        count = 1\n",
        "    else:\n",
        "        count += 1\n",
        "\n",
        "if current_asin is not None:\n",
        "    print(f\"{current_asin}\\t{count}\")"
      ],
      "metadata": {
        "id": "tuplTw4FWTwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
        "  -input movies.json \\\n",
        "  -output output1 \\\n",
        "  -mapper mapper1.py \\\n",
        "  -reducer reducer1.py \\\n",
        "  -file mapper1.py \\\n",
        "  -file reducer1.py"
      ],
      "metadata": {
        "id": "wx1MKlWJYx5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Task 2 — Compute Average Rating per Movie</h2>\n",
        "\n",
        "<p>\n",
        "In this task, you will compute the <b>average rating</b> for each movie.\n",
        "</p>\n",
        "\n",
        "<p>The mapper should:</p>\n",
        "<ul>\n",
        "  <li>Extract <code>asin</code> and <code>overall</code> (rating)</li>\n",
        "  <li>Emit <code>(asin, rating)</code></li>\n",
        "</ul>\n",
        "\n",
        "<p>The reducer should:</p>\n",
        "<ul>\n",
        "  <li>Sum all ratings for each movie</li>\n",
        "  <li>Count how many ratings were received</li>\n",
        "  <li>Compute and output the average rating</li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "Use a MapReduce job to generate a list of movies with their average ratings.\n",
        "</p>"
      ],
      "metadata": {
        "id": "hmOQLM5NwCU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your Mapper and Reducer code for Task 2 here.\n",
        "# You may use Python Hadoop Streaming or Java MapReduce."
      ],
      "metadata": {
        "id": "2UFxH-QuwFr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper2.py\n",
        "#!/usr/bin/env python3\n",
        "import sys,json\n",
        "\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    if not line:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        fichier = json.loads(line)\n",
        "        asin = fichier.get(\"asin\")\n",
        "        rating = fichier.get(\"overall\")\n",
        "\n",
        "        if asin is not None and rating is not None:\n",
        "            print(f\"{asin}\\t{rating}\")\n",
        "    except json.JSONDecodeError:\n",
        "        continue"
      ],
      "metadata": {
        "id": "mfqV6C8uWdIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer2.py\n",
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "\n",
        "current_asin = None\n",
        "count = 0\n",
        "rate = 0.0\n",
        "\n",
        "for line in sys.stdin:\n",
        "    asin, value = line.strip().split(\"\\t\")\n",
        "    value = float(value)\n",
        "\n",
        "    if asin != current_asin:\n",
        "        if current_asin is not None:\n",
        "            print(f\"{current_asin}\\t{rate/count}\")\n",
        "\n",
        "        current_asin = asin\n",
        "        count = 1\n",
        "        rate = value\n",
        "    else:\n",
        "        count += 1\n",
        "        rate += value\n",
        "\n",
        "if current_asin is not None:\n",
        "    print(f\"{current_asin}\\t{rate/count}\")"
      ],
      "metadata": {
        "id": "vfiE11aJWmkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
        "  -input movies.json \\\n",
        "  -output output2 \\\n",
        "  -mapper mapper2.py \\\n",
        "  -reducer reducer2.py \\\n",
        "  -file mapper2.py \\\n",
        "  -file reducer2.py"
      ],
      "metadata": {
        "id": "N1ImXAXGYsze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Task 3 — Extract Frequent Keywords from Reviews</h2>\n",
        "\n",
        "<p>\n",
        "Now you will perform text analysis on the <code>reviewText</code> field.\n",
        "Your task is to extract meaningful keywords for each movie.\n",
        "</p>\n",
        "\n",
        "<p>The mapper should:</p>\n",
        "<ul>\n",
        "  <li>Clean and tokenize the text</li>\n",
        "  <li>Remove punctuation and stopwords</li>\n",
        "  <li>Emit <code>(asin:word, 1)</code> for each keyword</li>\n",
        "</ul>\n",
        "\n",
        "<p>The reducer should:</p>\n",
        "<ul>\n",
        "  <li>Sum the counts for each <code>(asin, word)</code> pair</li>\n",
        "  <li>Output the total frequency of each keyword per movie</li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "This task combines text preprocessing with distributed computation.\n",
        "</p>"
      ],
      "metadata": {
        "id": "Z3nWiwDPwYjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your Mapper and Reducer code for Task 3 here."
      ],
      "metadata": {
        "id": "1BMjhTFXwXJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper3.py\n",
        "#!/usr/bin/env python3\n",
        "import sys,json,re\n",
        "\n",
        "STOPWORDS = {\n",
        "    \"i\",\"you\",\"he\",\"she\",\"it\",\"we\",\"they\",\"me\",\"him\",\"her\",\"us\",\"them\",\"my\",\n",
        "    \"your\",\"his\",\"her\",\"its\",\"our\",\"their\",\"mine\",\"yours\",\"hers\",\"ours\",\n",
        "    \"theirs\",\"the\",\"a\",\"an\",\"and\",\"or\",\"but\",\"so\",\"yet\",\"nor\",\"for\",\"in\",\"on\",\n",
        "    \"at\",\"by\",\"for\",\"from\",\"to\",\"of\",\"with\",\"without\",\"into\",\"onto\",\"over\",\n",
        "    \"very\",\"too\",\"just\",\"only\",\"also\",\"again\",\"still\",\"even\",\"ever\",\"never\",\n",
        "    \"be\",\"am\",\"is\",\"are\",\"was\",\"were\",\"been\",\"being\",\"have\",\"has\",\"had\",\n",
        "    \"having\",\"do\",\"does\",\"did\",\"doing\",\"can\",\"could\",\"shall\",\"should\",\"will\",\n",
        "    \"would\",\"may\",\"might\",\"must\",\"get\",\"gets\",\"got\",\"getting\",\"make\",\"makes\",\n",
        "    \"made\",\"making\",\"go\",\"goes\",\"went\",\"gone\",\"going\",\"see\",\"sees\",\"saw\",\"seen\",\n",
        "    \"seeing\",\"know\",\"knows\",\"knew\",\"known\",\"knowing\",\"this\",\"that\",\"these\",\n",
        "    \"those\",\"there\",\"here\",\"where\",\"when\",\"why\",\"how\",\"what\",\"which\",\"who\",\n",
        "    \"whom\",\"whose\",\"such\",\"many\",\"much\",\"more\",\"most\",\"some\",\"any\",\"all\",\"each\",\n",
        "    \"both\",\"either\",\"neither\",\"because\",\"though\",\"although\",\"while\",\"unless\",\n",
        "    \"since\",\"until\",\"before\",\"after\",\"about\",\n",
        "}\n",
        "\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    if not line:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        data = json.loads(line)\n",
        "        asin = data.get(\"asin\")\n",
        "        text = data.get(\"reviewText\", \"\")\n",
        "\n",
        "        if not asin or not text:\n",
        "            continue\n",
        "\n",
        "        # lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # remove punctuation using regex\n",
        "        text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
        "\n",
        "        # tokenize\n",
        "        words = text.split()\n",
        "\n",
        "        for word in words:\n",
        "            if len(word) > 2 and word not in STOPWORDS:\n",
        "                print(f\"{asin}:{word}\\t1\")\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        continue"
      ],
      "metadata": {
        "id": "kM8RKl8qXJhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer3.py\n",
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "\n",
        "current_key = None\n",
        "count = 0\n",
        "\n",
        "for line in sys.stdin:\n",
        "    key, value = line.strip().split(\"\\t\")\n",
        "    value = int(value)\n",
        "\n",
        "    if key != current_key:\n",
        "        if current_key is not None:\n",
        "            print(f\"{current_key}\\t{count}\")\n",
        "        current_key = key\n",
        "        count = 1\n",
        "    else:\n",
        "        count += 1\n",
        "\n",
        "# last key\n",
        "if current_key is not None:\n",
        "    print(f\"{current_key}\\t{count}\")"
      ],
      "metadata": {
        "id": "mGlsKS-JYWgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
        "  -input movies.json \\\n",
        "  -output output3 \\\n",
        "  -mapper mapper3.py \\\n",
        "  -reducer reducer3.py \\\n",
        "  -file mapper3.py \\\n",
        "  -file reducer3.py"
      ],
      "metadata": {
        "id": "JeE3kvAyYfkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Task 4 — Join Ratings with Top Keywords</h2>\n",
        "\n",
        "<p>\n",
        "For this task, you will combine the results of Task 2 (average ratings) and\n",
        "Task 3 (keyword frequencies) using a <b>reduce-side join</b>.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "You will provide two inputs to your MapReduce job:\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "  <li><b>Ratings file</b> with <code>(asin, average_rating)</code></li>\n",
        "  <li><b>Keywords file</b> with <code>(asin, keyword, count)</code></li>\n",
        "</ul>\n",
        "\n",
        "<p>Each mapper should tag its data:</p>\n",
        "\n",
        "<ul>\n",
        "  <li><code>(\"R\", rating)</code> for ratings</li>\n",
        "  <li><code>(\"K\", keyword:count)</code> for keywords</li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "The reducer will receive all entries for a given movie and combine them to\n",
        "produce an output containing:\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "  <li>The movie identifier (<code>asin</code>)</li>\n",
        "  <li>Its average rating</li>\n",
        "  <li>Its most frequent keywords</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "FVEbnUFtwejW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your Mapper and Reducer code for Task 4 here."
      ],
      "metadata": {
        "id": "y7h5RAbLwigt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}